from openai import OpenAI
import os
from dotenv import load_dotenv
from typing import List, Dict, Optional
import tiktoken

load_dotenv()


class LLMService:
    def __init__(self):
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY")
        )
        self.quick_responses = {
            '–ø—Ä–∏–≤–µ—Ç': '–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø –≤–∞—à –±–∏–∑–Ω–µ—Å-–ø–æ–º–æ—â–Ω–∏–∫. –ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É, —Ñ–∏–Ω–∞–Ω—Å–∞–º, —é—Ä–∏—Å–ø—Ä—É–¥–µ–Ω—Ü–∏–∏ –∏–ª–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –±–∏–∑–Ω–µ—Å–æ–º.',
            '—Å–ø–∞—Å–∏–±–æ': '–ü–æ–∂–∞–ª—É–π—Å—Ç–∞! –û–±—Ä–∞—â–∞–π—Ç–µ—Å—å, –µ—Å–ª–∏ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –µ—â—ë –ø–æ–º–æ—â—å.',
            '–ø–æ–º–æ—â—å': '–Ø –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É—é –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º –±–∏–∑–Ω–µ—Å–∞: –º–∞—Ä–∫–µ—Ç–∏–Ω–≥, —Ñ–∏–Ω–∞–Ω—Å—ã, —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã, —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ. –ó–∞–¥–∞–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å!',
        }
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
        try:
            self.encoding = tiktoken.get_encoding("cl100k_base")
        except:
            self.encoding = None

    def get_quick_response(self, question: str) -> Optional[str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –±—ã—Å—Ç—Ä—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤"""
        return self.quick_responses.get(question.lower().strip())

    def count_tokens(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if not self.encoding:
            return len(text.split())  # fallback
        return len(self.encoding.encode(text))

    def prepare_conversation_messages(
            self,
            system_prompt: str,
            user_question: str,
            conversation_history: List[Dict] = None,
            max_tokens: int = 3000
    ) -> List[Dict]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è LLM —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ —Ç–æ–∫–µ–Ω–∞–º
        """
        messages = [{"role": "system", "content": system_prompt}]
        current_tokens = self.count_tokens(system_prompt)

        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if conversation_history:
            # –ò–¥–µ–º –æ—Ç —Å–∞–º—ã—Ö —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º, –Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º
            history_messages = []
            history_tokens = 0

            for msg in reversed(conversation_history):  # –Ω–∞—á–∏–Ω–∞–µ–º —Å —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö
                if hasattr(msg, 'role') and hasattr(msg, 'content'):
                    # –ï—Å–ª–∏ —ç—Ç–æ SQLAlchemy –æ–±—ä–µ–∫—Ç
                    role = msg.role
                    content = msg.content
                elif isinstance(msg, dict) and 'role' in msg and 'content' in msg:
                    # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å
                    role = msg['role']
                    content = msg['content']
                else:
                    continue

                message_tokens = self.count_tokens(content)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—Å–∏–º –ª–∏ –ª–∏–º–∏—Ç
                if current_tokens + history_tokens + message_tokens > max_tokens:
                    break

                history_messages.insert(0, {"role": role, "content": content})
                history_tokens += message_tokens

            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é
            messages.extend(history_messages)
            current_tokens += history_tokens

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_tokens = self.count_tokens(user_question)
        messages.append({"role": "user", "content": user_question})
        current_tokens += user_tokens

        print(
            f"üìä –¢–æ–∫–µ–Ω—ã: —Å–∏—Å—Ç–µ–º–∞={self.count_tokens(system_prompt)}, –∏—Å—Ç–æ—Ä–∏—è={current_tokens - self.count_tokens(system_prompt) - user_tokens}, –≤–æ–ø—Ä–æ—Å={user_tokens}, –≤—Å–µ–≥–æ={current_tokens}")

        return messages

    def generate_response(
            self,
            system_prompt: str,
            user_question: str,
            conversation_history: List[Dict] = None,
            max_history_tokens: int = 3000
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π

        Args:
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            user_question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            conversation_history: –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π (–∏–∑ –ë–î)
            max_history_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏

        Returns:
            –û—Ç–≤–µ—Ç –æ—Ç LLM –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ —Ç–æ–∫–µ–Ω–∞–º
            messages = self.prepare_conversation_messages(
                system_prompt,
                user_question,
                conversation_history,
                max_history_tokens
            )

            completion = self.client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "http://localhost:5000",
                    "X-Title": "Business Assistant",
                },
                model="tngtech/deepseek-r1t2-chimera:free",
                messages=messages,
                temperature=0.5,
                max_tokens=1000  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –æ—Ç–≤–µ—Ç
            )

            if not completion.choices or len(completion.choices) == 0:
                raise ValueError("LLM –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")

            response = completion.choices[0].message.content
            
            if not response:
                raise ValueError("LLM –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ")

            return response

        except ValueError as e:
            raise
        except Exception as e:
            error_message = str(e)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ 401 - –Ω–µ–≤–µ—Ä–Ω—ã–π API –∫–ª—é—á
            if "401" in error_message or "User not found" in error_message or "authentication" in error_message.lower():
                raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π API –∫–ª—é—á OpenRouter. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é OPENROUTER_API_KEY.")
            elif "rate limit" in error_message.lower() or "quota" in error_message.lower() or "429" in error_message:
                raise ValueError("–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
            elif "timeout" in error_message.lower():
                raise ValueError("–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑.")
            else:
                raise ValueError(f"–û—à–∏–±–∫–∞ LLM: {error_message}")

    def generate_response_with_context(
            self,
            system_prompt: str,
            context_messages: List[Dict],
            user_question: str
    ) -> str:
        """
        –£—Å—Ç–∞—Ä–µ–≤—à–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        """
        return self.generate_response(system_prompt, user_question, context_messages)

    def summarize_conversation(self, conversation_history: List[Dict]) -> str:
        """
        –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω–Ω–æ–π –±–µ—Å–µ–¥—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Args:
            conversation_history: –ü–æ–ª–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –±–µ—Å–µ–¥—ã

        Returns:
            –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –±–µ—Å–µ–¥—ã
        """
        if not conversation_history or len(conversation_history) < 5:
            return ""

        try:
            summary_prompt = """
            –°—É–º–º–∞—Ä–∏–∑—É–π —Å–ª–µ–¥—É—é—â—É—é –±–µ—Å–µ–¥—É –≤ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö, –≤—ã–¥–µ–ª–∏–≤ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è.
            –°–æ—Ö—Ä–∞–Ω–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –±—É–¥—É—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.

            –ë–µ—Å–µ–¥–∞:
            """

            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            recent_history = conversation_history[-10:]  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π

            conversation_text = ""
            for msg in recent_history:
                if hasattr(msg, 'role') and hasattr(msg, 'content'):
                    role = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" if msg.role == "user" else "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç"
                    conversation_text += f"{role}: {msg.content}\n"
                elif isinstance(msg, dict):
                    role = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" if msg.get('role') == "user" else "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç"
                    conversation_text += f"{role}: {msg.get('content', '')}\n"

            summary_prompt += conversation_text

            completion = self.client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "http://localhost:5000",
                    "X-Title": "Business Assistant",
                },
                model="tngtech/deepseek-r1t2-chimera:free",
                messages=[
                    {"role": "system", "content": "–¢—ã –ø–æ–º–æ–≥–∞–µ—à—å —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–µ—Å–µ–¥—ã."},
                    {"role": "user", "content": summary_prompt}
                ],
                temperature=0.3,
                max_tokens=300
            )

            return completion.choices[0].message.content

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return ""

    def get_conversation_stats(self, conversation_history: List[Dict]) -> Dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –±–µ—Å–µ–¥–µ

        Args:
            conversation_history: –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        total_messages = len(conversation_history)
        user_messages = 0
        assistant_messages = 0
        total_tokens = 0

        for msg in conversation_history:
            if hasattr(msg, 'role') and hasattr(msg, 'content'):
                role = msg.role
                content = msg.content
            elif isinstance(msg, dict):
                role = msg.get('role', '')
                content = msg.get('content', '')
            else:
                continue

            if role == 'user':
                user_messages += 1
            elif role == 'assistant':
                assistant_messages += 1

            total_tokens += self.count_tokens(content)

        return {
            'total_messages': total_messages,
            'user_messages': user_messages,
            'assistant_messages': assistant_messages,
            'estimated_tokens': total_tokens,
            'conversation_ratio': user_messages / total_messages if total_messages > 0 else 0
        }