import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import joblib
import re
# from sentence_transformers import SentenceTransformer


class EnhancedBusinessClassifier:
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        # self.embedder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        self.classifier = RandomForestClassifier(
            n_estimators=100,
            max_depth=20,
            min_samples_split=3,
            min_samples_leaf=1,
            random_state=42
        )
        self.labels = ['marketing', 'finance', 'legal', 'management', 'sales', 'general']

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        self.category_keywords = {
            'marketing': ['–º–∞—Ä–∫–µ—Ç–∏–Ω–≥', '—Ä–µ–∫–ª–∞–º–∞', '–ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ', '–±—Ä–µ–Ω–¥', 'smm', 'seo', '—Ç–∞—Ä–≥–µ—Ç–∏–Ω–≥',
                          '–∫–æ–Ω—Ç–µ–Ω—Ç', '–∞—É–¥–∏—Ç–æ—Ä–∏—è', '—Ç—Ä–∞—Ñ–∏–∫', '–∫–æ–Ω–≤–µ—Ä—Å–∏—è', '–≤–æ—Ä–æ–Ω–∫–∞'],
            'finance': ['—Ñ–∏–Ω–∞–Ω—Å', '–±—é–¥–∂–µ—Ç', '–Ω–∞–ª–æ–≥', '–∏–Ω–≤–µ—Å—Ç–∏—Ü', '–∫—Ä–µ–¥–∏—Ç', '–¥–µ–Ω—å–≥–∏', '–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å',
                        '—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å', '–≤—ã—Ä—É—á–∫–∞', '–ø—Ä–∏–±—ã–ª—å', '—Ä–∞—Å—Ö–æ–¥', '–∫–∞—Å—Å–∞'],
            'legal': ['—é—Ä–∏–¥–∏—á', '–¥–æ–≥–æ–≤–æ—Ä', '–ø—Ä–∞–≤–æ–≤', '–∑–∞–∫–æ–Ω', '–ª–∏—Ü–µ–Ω–∑', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü', '—Å—É–¥',
                      '–∏—Å–∫', '–∞–¥–≤–æ–∫–∞—Ç', '–Ω–æ—Ç–∞—Ä–∏—É—Å', '–ø–∞—Ç–µ–Ω—Ç', '–∞–≤—Ç–æ—Ä—Å–∫–æ–µ'],
            'management': ['—É–ø—Ä–∞–≤–ª–µ–Ω', '–∫–æ–º–∞–Ω–¥–∞', '–ø–µ—Ä—Å–æ–Ω–∞–ª', '–ø—Ä–æ—Ü–µ—Å—Å', '–æ–ø—Ç–∏–º–∏–∑–∞—Ü', 'kpi',
                           '–º–æ—Ç–∏–≤–∞—Ü', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤', '–æ—Ç–¥–µ–ª', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å'],
            'sales': ['–ø—Ä–æ–¥–∞–∂', '–∫–ª–∏–µ–Ω—Ç', '—Å–¥–µ–ª–∫–∞', '–ª–∏–¥', 'crm', '–≤–æ–∑—Ä–∞–∂–µ–Ω', '–∫–æ–º–º–µ—Ä—á–µ—Å–∫',
                      '–∫–æ–Ω—Ç—Ä–∞–∫—Ç', '–º–µ–Ω–µ–¥–∂–µ—Ä', '–∑–∞–ø—Ä–æ—Å', '–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'],
            'general': ['–±–∏–∑–Ω–µ—Å', '—Å—Ç–∞—Ä—Ç–∞–ø', '–∫–æ–º–ø–∞–Ω–∏—è', '—Ä–∞–∑–≤–∏—Ç–∏–µ', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è', '–ø–ª–∞–Ω',
                        '–∏–¥–µ—è', '–ø—Ä–æ–µ–∫—Ç', '—Ä—ã–Ω–æ–∫', '–∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è', '–Ω–∏—à–∞']
        }

    def preprocess_text(self, text):
        """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞"""
        text = str(text).lower()
        text = re.sub(r'[^\w\s#+]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def calculate_keyword_features(self, text):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        text_lower = text.lower()
        features = []

        for category, keywords in self.category_keywords.items():
            # 1. –ü—Ä–æ—Å—Ç–æ–µ –Ω–∞–ª–∏—á–∏–µ
            presence = sum(1 for keyword in keywords if keyword in text_lower)

            # 2. –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –ø–æ TF (—á–∞—Å—Ç–æ—Ç–µ –≤ —Ç–µ–∫—Å—Ç–µ)
            tf_score = sum(text_lower.count(keyword) for keyword in keywords)

            # 3. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å—á–µ—Ç
            normalized_score = tf_score / max(len(text.split()), 1)

            features.extend([presence, tf_score, normalized_score])

        return np.array(features)

    def get_text_embeddings(self, texts):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if isinstance(texts, str):
            texts = [texts]

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings = self.embedder.encode(texts, convert_to_tensor=False)
        return embeddings

    def extract_text_features(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∏—á"""
        words = text.split()
        sentences = text.split('.')

        features = [
            len(text),  # –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
            len(words),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
            len(sentences),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            np.mean([len(word) for word in words]) if words else 0,  # —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞
            len([w for w in words if len(w) > 6]) / max(len(words), 1),  # –¥–æ–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤
        ]

        return np.array(features)

    def train(self, dataset):
        df = pd.DataFrame(dataset)
        df['processed_text'] = df['text'].apply(self.preprocess_text)

        print("üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        # 1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings = self.get_text_embeddings(df['processed_text'].tolist())

        # 2. –ü—Ä–∏–∑–Ω–∞–∫–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keyword_features = np.array([self.calculate_keyword_features(text) for text in df['text']])

        # 3. –¢–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        text_metrics = np.array([self.extract_text_features(text) for text in df['processed_text']])

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        X_combined = np.hstack([embeddings, keyword_features, text_metrics])
        y = df['label']

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X_combined, y, test_size=0.15, random_state=42, stratify=y
        )

        print(f"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_combined.shape}")

        # –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        self.classifier.fit(X_train, y_train)

        # –û—Ü–µ–Ω–∫–∞
        train_score = self.classifier.score(X_train, y_train)
        test_score = self.classifier.score(X_test, y_test)

        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ: {train_score:.3f}")
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {test_score:.3f}")

        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        y_pred = self.classifier.predict(X_test)
        print("\nüìã –î–µ—Ç–∞–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞:")
        print(classification_report(y_test, y_pred, target_names=self.labels))

        return train_score, test_score

    def predict(self, text):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏"""
        processed_text = self.preprocess_text(text)

        # 1. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        embedding = self.get_text_embeddings(processed_text)

        # 2. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keyword_feats = self.calculate_keyword_features(text).reshape(1, -1)

        # 3. –¢–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        text_feats = self.extract_text_features(processed_text).reshape(1, -1)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º
        combined_features = np.hstack([embedding, keyword_feats, text_feats])

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = self.classifier.predict(combined_features)[0]
        probabilities = self.classifier.predict_proba(combined_features)[0]

        prob_dict = {
            label: round(prob, 3)
            for label, prob in zip(self.classifier.classes_, probabilities)
        }

        return prediction, prob_dict

    def save_model(self, path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        model_data = {
            'classifier': self.classifier,
            'labels': self.labels,
            'category_keywords': self.category_keywords
        }
        joblib.dump(model_data, path)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {path}")

    def load_model(self, path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        model_data = joblib.load(path)
        self.classifier = model_data['classifier']
        self.labels = model_data['labels']
        self.category_keywords = model_data['category_keywords']
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {path}")