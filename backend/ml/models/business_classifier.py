import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import joblib
import re


class EnhancedBusinessClassifier:
    def __init__(self):
        self.classifier = RandomForestClassifier(
            n_estimators=100,
            max_depth=30,
            min_samples_split=3,
            min_samples_leaf=1,
            random_state=42
        )
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤–∫–ª—é—á–∞—è 'graphic'
        self.labels = ['marketing', 'finance', 'legal', 'management', 'sales', 'general', 'graphic']

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        self.category_keywords = {
            'marketing': ['–º–∞—Ä–∫–µ—Ç–∏–Ω–≥', '—Ä–µ–∫–ª–∞–º–∞', '–ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ', '–±—Ä–µ–Ω–¥', 'smm', 'seo', '—Ç–∞—Ä–≥–µ—Ç–∏–Ω–≥',
                          '–∫–æ–Ω—Ç–µ–Ω—Ç', '–∞—É–¥–∏—Ç–æ—Ä–∏—è', '—Ç—Ä–∞—Ñ–∏–∫', '–∫–æ–Ω–≤–µ—Ä—Å–∏—è', '–≤–æ—Ä–æ–Ω–∫–∞'],
            'finance': ['—Ñ–∏–Ω–∞–Ω—Å', '–±—é–¥–∂–µ—Ç', '–Ω–∞–ª–æ–≥', '–∏–Ω–≤–µ—Å—Ç–∏—Ü', '–∫—Ä–µ–¥–∏—Ç', '–¥–µ–Ω—å–≥–∏', '–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å',
                        '—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å', '–≤—ã—Ä—É—á–∫–∞', '–ø—Ä–∏–±—ã–ª—å', '—Ä–∞—Å—Ö–æ–¥', '–∫–∞—Å—Å–∞'],
            'legal': ['—é—Ä–∏–¥–∏—á', '–¥–æ–≥–æ–≤–æ—Ä', '–ø—Ä–∞–≤–æ–≤', '–∑–∞–∫–æ–Ω', '–ª–∏—Ü–µ–Ω–∑', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü', '—Å—É–¥',
                      '–∏—Å–∫', '–∞–¥–≤–æ–∫–∞—Ç', '–Ω–æ—Ç–∞—Ä–∏—É—Å', '–ø–∞—Ç–µ–Ω—Ç', '–∞–≤—Ç–æ—Ä—Å–∫–æ–µ'],
            'management': ['—É–ø—Ä–∞–≤–ª–µ–Ω', '–∫–æ–º–∞–Ω–¥–∞', '–ø–µ—Ä—Å–æ–Ω–∞–ª', '–ø—Ä–æ—Ü–µ—Å—Å', '–æ–ø—Ç–∏–º–∏–∑–∞—Ü', 'kpi',
                           '–º–æ—Ç–∏–≤–∞—Ü', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤', '–æ—Ç–¥–µ–ª', '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å'],
            'sales': ['–ø—Ä–æ–¥–∞–∂', '–∫–ª–∏–µ–Ω—Ç', '—Å–¥–µ–ª–∫–∞', '–ª–∏–¥', 'crm', '–≤–æ–∑—Ä–∞–∂–µ–Ω', '–∫–æ–º–º–µ—Ä—á–µ—Å–∫',
                      '–∫–æ–Ω—Ç—Ä–∞–∫—Ç', '–º–µ–Ω–µ–¥–∂–µ—Ä', '–∑–∞–ø—Ä–æ—Å', '–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ'],
            'general': ['–±–∏–∑–Ω–µ—Å', '—Å—Ç–∞—Ä—Ç–∞–ø', '–∫–æ–º–ø–∞–Ω–∏—è', '—Ä–∞–∑–≤–∏—Ç–∏–µ', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è', '–ø–ª–∞–Ω',
                        '–∏–¥–µ—è', '–ø—Ä–æ–µ–∫—Ç', '—Ä—ã–Ω–æ–∫', '–∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è', '–Ω–∏—à–∞'],
            'graphic': [
                '–≥—Ä–∞—Ñ–∏–∫', '–¥–∏–∞–≥—Ä–∞–º–º–∞', '–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü', 'chart', 'plot', '–≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å',
                '–æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å', '–ø–æ—Å—Ç—Ä–æ–∏—Ç—å', '–Ω–∞—Ä–∏—Å–æ–≤–∞—Ç—å', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫', '–¥–∞–Ω–Ω—ã–µ', '—Ç–∞–±–ª–∏—Ü',
                '–∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö', '–≥–∏—Å—Ç–æ–≥—Ä–∞–º–º', '–∫—Ä—É–≥–æ–≤–∞—è', '—Å—Ç–æ–ª–±—á–∞—Ç–∞—è', '–ª–∏–Ω–µ–π–Ω—ã–π', 'scatter',
                'heatmap', '—Ç—Ä–µ–Ω–¥', '–¥–∏–Ω–∞–º–∏–∫–∞', '—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', '–¥–æ–ª—è', '–ø—Ä–æ—Ü–µ–Ω—Ç', '–≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞',
                '—Ç–æ—á–µ—á–Ω—ã–π', '–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è', '–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å', '—è—â–∏–∫ —Å —É—Å–∞–º–∏', 'box plot', 'violin',
                'bar chart', 'pie chart', 'line chart', 'area chart', '–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π', '–≤–∏–∑—É–∞–ª—å–Ω—ã–π',
                '–æ—Ç—á—ë—Ç', '–æ—Ç—á–µ—Ç', '–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è', '–¥–∏–∞–≥—Ä–∞–º', '–ø–æ—Å—Ç—Ä–æ–π', '–Ω–∞—Ä–∏—Å—É–π', '–ø–æ–∫–∞–∂–∏',
                '—Å–æ–∑–¥–∞–π', '–æ—Ç–æ–±—Ä–∞–∑–∏', '–ø—Ä–µ–¥—Å—Ç–∞–≤—å –≤ –≤–∏–¥–µ'
            ]
        }

    def preprocess_text(self, text):
        """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞"""
        text = str(text).lower()
        text = re.sub(r'[^\w\s#+]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def calculate_keyword_features(self, text):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        text_lower = text.lower()
        features = []

        for category, keywords in self.category_keywords.items():
            presence = sum(1 for keyword in keywords if keyword in text_lower)
            tf_score = sum(text_lower.count(keyword) for keyword in keywords)
            normalized_score = tf_score / max(len(text.split()), 1)
            features.extend([presence, tf_score, normalized_score])

        return np.array(features)

    def get_text_embeddings(self, texts):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–∑–∞–≥–ª—É—à–∫–∞)"""
        if isinstance(texts, str):
            texts = [texts]

        embedding_dim = 384
        if len(texts) > 0:
            embeddings = np.zeros((len(texts), embedding_dim))
        else:
            embeddings = np.array([]).reshape(0, embedding_dim)

        return embeddings

    def extract_text_features(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∏—á"""
        words = text.split()
        sentences = text.split('.')

        features = [
            len(text),
            len(words),
            len(sentences),
            np.mean([len(word) for word in words]) if words else 0,
            len([w for w in words if len(w) > 6]) / max(len(words), 1),
        ]

        return np.array(features)

    def train(self, dataset):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        df = pd.DataFrame(dataset)
        df['processed_text'] = df['text'].apply(self.preprocess_text)

        print("üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        print(f"üìä –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {df['label'].unique()}")
        print(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {df['label'].value_counts().to_dict()}")

        # 1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–∑–∞–≥–ª—É—à–∫–∞)
        embeddings = self.get_text_embeddings(df['processed_text'].tolist())

        # 2. –ü—Ä–∏–∑–Ω–∞–∫–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (—Ç–µ–ø–µ—Ä—å 7 –∫–∞—Ç–µ–≥–æ—Ä–∏–π √ó 3 —Ñ–∏—á–∏ = 21 —Ñ–∏—á–∞)
        keyword_features = np.array([self.calculate_keyword_features(text) for text in df['text']])

        # 3. –¢–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        text_metrics = np.array([self.extract_text_features(text) for text in df['processed_text']])

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        X_combined = np.hstack([embeddings, keyword_features, text_metrics])
        y = df['label']

        print(f"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_combined.shape}")
        print(f"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embeddings.shape}")
        print(f"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å keyword_features: {keyword_features.shape}")
        print(f"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å text_metrics: {text_metrics.shape}")

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test —Å —É—á–µ—Ç–æ–º –Ω–æ–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        X_train, X_test, y_train, y_test = train_test_split(
            X_combined, y, test_size=0.15, random_state=42, stratify=y
        )

        # –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        self.classifier.fit(X_train, y_train)

        # –û—Ü–µ–Ω–∫–∞
        train_score = self.classifier.score(X_train, y_train)
        test_score = self.classifier.score(X_test, y_test)

        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ: {train_score:.3f}")
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {test_score:.3f}")

        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        y_pred = self.classifier.predict(X_test)
        print("\nüìã –î–µ—Ç–∞–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞:")
        print(classification_report(y_test, y_pred, target_names=self.labels))

        return train_score, test_score

    def predict(self, text):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏"""
        processed_text = self.preprocess_text(text)

        # 1. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ (–∑–∞–≥–ª—É—à–∫–∞)
        embedding = self.get_text_embeddings(processed_text)

        # 2. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keyword_feats = self.calculate_keyword_features(text).reshape(1, -1)

        # 3. –¢–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        text_feats = self.extract_text_features(processed_text).reshape(1, -1)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º
        combined_features = np.hstack([embedding, keyword_feats, text_feats])

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = self.classifier.predict(combined_features)[0]
        probabilities = self.classifier.predict_proba(combined_features)[0]

        prob_dict = {
            label: round(prob, 3)
            for label, prob in zip(self.classifier.classes_, probabilities)
        }

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        print(f"üîç –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è '{text[:50]}...': {prediction}")
        print(f"üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {prob_dict}")

        return prediction, prob_dict

    def save_model(self, path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        model_data = {
            'classifier': self.classifier,
            'labels': self.labels,
            'category_keywords': self.category_keywords
        }
        joblib.dump(model_data, path)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {path}")

    def load_model(self, path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        model_data = joblib.load(path)
        self.classifier = model_data['classifier']
        self.labels = model_data['labels']
        self.category_keywords = model_data['category_keywords']
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {path}")