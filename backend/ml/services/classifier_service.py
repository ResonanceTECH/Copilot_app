import os
import joblib
import numpy as np
import re
from typing import Dict, Any, Tuple


class BusinessClassifierService:
    def __init__(self, model_path: str = None):
        if model_path is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            ml_dir = os.path.dirname(current_dir)
            model_path = os.path.join(ml_dir, "models", "business_classifier.pkl")

        self.model_path = model_path
        self.model_data = None
        self.classifier = None
        self.labels = ['marketing', 'finance', 'legal', 'management', 'sales', 'general']
        self.category_keywords = {}
        self.load_model()

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–∑ PKL —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists(self.model_path):
                self.model_data = joblib.load(self.model_path)
                print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ –∏–∑ PKL")

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏
                self.classifier = self.model_data.get('classifier')
                self.labels = self.model_data.get('labels', self.labels)
                self.category_keywords = self.model_data.get('category_keywords', {})

                self._print_model_info()

            else:
                print(f"‚ö†Ô∏è  –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.model_path}")
                self.model_data = None

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            import traceback
            traceback.print_exc()
            self.model_data = None

    def _print_model_info(self):
        """–í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if not self.model_data:
            return

        print("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏:")
        print(f"   üéØ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä: {self.classifier.__class__.__name__}")
        print(f"   üè∑  –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {self.labels}")
        print(f"   üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {len(self.category_keywords)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")

        if hasattr(self.classifier, 'n_features_in_'):
            print(f"   üìè –û–∂–∏–¥–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {self.classifier.n_features_in_}")
        if hasattr(self.classifier, 'n_estimators'):
            print(f"   üå≥ –î–µ—Ä–µ–≤—å—è: {self.classifier.n_estimators}")

    def _calculate_keyword_features(self, text: str) -> np.ndarray:
        """–§–∏—á–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–¢–û–ß–ù–û –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)"""
        text_lower = text.lower()
        features = []

        for category, keywords in self.category_keywords.items():
            # 1. –ü—Ä–æ—Å—Ç–æ–µ –Ω–∞–ª–∏—á–∏–µ
            presence = sum(1 for keyword in keywords if keyword in text_lower)

            # 2. –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –ø–æ TF (—á–∞—Å—Ç–æ—Ç–µ –≤ —Ç–µ–∫—Å—Ç–µ)
            tf_score = sum(text_lower.count(keyword) for keyword in keywords)

            # 3. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å—á–µ—Ç
            normalized_score = tf_score / max(len(text.split()), 1)

            features.extend([presence, tf_score, normalized_score])

        return np.array(features)

    def _extract_text_features(self, text: str) -> np.ndarray:
        """–¢–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–¢–û–ß–ù–û –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)"""
        words = text.split()
        sentences = text.split('.')

        features = [
            len(text),  # –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
            len(words),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
            len(sentences),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            np.mean([len(word) for word in words]) if words else 0,  # —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞
            len([w for w in words if len(w) > 6]) / max(len(words), 1),  # –¥–æ–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤
        ]

        return np.array(features)

    def _create_zero_embeddings(self) -> np.ndarray:
        """–°–æ–∑–¥–∞–µ–º –Ω—É–ª–µ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (384 —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è MiniLM-L12-v2)"""
        return np.zeros(384)

    def _prepare_features(self, text: str) -> np.ndarray:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        # 1. –ù–£–õ–ï–í–´–ï —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (—Ç–∞–∫ –∫–∞–∫ embedder –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω)
        embeddings = self._create_zero_embeddings()

        # 2. –ü—Ä–∏–∑–Ω–∞–∫–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keyword_features = self._calculate_keyword_features(text)

        # 3. –¢–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        text_features = self._extract_text_features(text)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        combined_features = np.hstack([embeddings, keyword_features, text_features])

        print(f"üìä –ü—Ä–∏–∑–Ω–∞–∫–∏: —ç–º–±–µ–¥–¥–∏–Ω–≥–∏=384, –∫–ª—é—á–∏={len(keyword_features)}, –º–µ—Ç—Ä–∏–∫–∏={len(text_features)}")

        return combined_features.reshape(1, -1)

    def predict_category(self, text: str) -> Tuple[str, Dict[str, float]]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤–æ–ø—Ä–æ—Å–∞"""
        if not self.model_data or not self.classifier:
            print("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ")
            return self._dummy_prediction(text)

        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = self._prepare_features(text)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            expected_features = self.classifier.n_features_in_
            actual_features = features.shape[1]

            if actual_features != expected_features:
                print(f"‚ö†Ô∏è  –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: –æ–∂–∏–¥–∞–ª–æ—Å—å {expected_features}, –ø–æ–ª—É—á–∏–ª–∏ {actual_features}")
                return self._dummy_prediction(text)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            prediction = self.classifier.predict(features)[0]
            probabilities = self.classifier.predict_proba(features)[0]

            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            prob_dict = {
                label: round(prob, 3)
                for label, prob in zip(self.classifier.classes_, probabilities)
            }

            print(f"üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: '{text[:50]}...' -> {prediction} (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {prob_dict[prediction]:.1%})")
            return prediction, prob_dict

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            import traceback
            traceback.print_exc()
            return self._keyword_based_prediction(text)

    def _keyword_based_prediction(self, text: str) -> Tuple[str, Dict[str, float]]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        keyword_features = self._calculate_keyword_features(text)

        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ presence —Ñ–∏—á–∏ (–ø–µ—Ä–≤—ã–µ 6 –∑–Ω–∞—á–µ–Ω–∏–π)
        presence_scores = keyword_features[::3][:6]

        max_score = max(presence_scores)
        if max_score > 0:
            best_category_idx = np.argmax(presence_scores)
            best_category = self.labels[best_category_idx]

            # –°–æ–∑–¥–∞–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            probs = {category: 0.05 for category in self.labels}
            probs[best_category] = 0.7
            return best_category, probs
        else:
            return 'general', {'general': 1.0}

    def _dummy_prediction(self, text: str) -> Tuple[str, Dict[str, float]]:
        """–§–∏–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"""
        text_lower = text.lower()

        keyword_mapping = {
            'marketing': ['–º–∞—Ä–∫–µ—Ç–∏–Ω–≥', '—Ä–µ–∫–ª–∞–º–∞', '–ø—Ä–æ–¥–≤–∏–∂–µ–Ω', '–±—Ä–µ–Ω–¥', 'smm', 'seo'],
            'finance': ['—Ñ–∏–Ω–∞–Ω—Å', '–±—é–¥–∂–µ—Ç', '–Ω–∞–ª–æ–≥', '–¥–µ–Ω—å–≥', '–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å', '—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å'],
            'legal': ['—é—Ä–∏–¥–∏—á', '–¥–æ–≥–æ–≤–æ—Ä', '–∑–∞–∫–æ–Ω', '–ø—Ä–∞–≤', '–ª–∏—Ü–µ–Ω–∑', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü'],
            'management': ['—É–ø—Ä–∞–≤–ª–µ–Ω', '–∫–æ–º–∞–Ω–¥', '–ø–µ—Ä—Å–æ–Ω–∞–ª', '–ø—Ä–æ—Ü–µ—Å—Å', 'kpi', '–º–æ—Ç–∏–≤–∞—Ü'],
            'sales': ['–ø—Ä–æ–¥–∞–∂', '–∫–ª–∏–µ–Ω—Ç', '—Å–¥–µ–ª–∫', '–ª–∏–¥', 'crm', '–≤–æ–∑—Ä–∞–∂–µ–Ω']
        }

        for category, keywords in keyword_mapping.items():
            if any(keyword in text_lower for keyword in keywords):
                probs = {category: 0.8, 'general': 0.2}
                return category, probs

        return 'general', {'general': 1.0}

    def is_ready(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞"""
        return self.model_data is not None and self.classifier is not None

    def get_model_info(self) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if not self.model_data:
            return {"status": "not_loaded"}

        info = {
            "status": "loaded",
            "classifier": self.classifier.__class__.__name__,
            "categories": self.labels,
            "keyword_categories": len(self.category_keywords)
        }

        if hasattr(self.classifier, 'n_features_in_'):
            info["expected_features"] = self.classifier.n_features_in_

        return info
